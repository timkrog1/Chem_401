{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing useful python packages\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7014d74",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\ket}[1]{\\left|{#1}\\right\\rangle}$$\n",
    "$$\\newcommand{\\bra}[1]{\\left\\langle{#1}\\right|}$$\n",
    "$$\\newcommand{\\braket}[2]{\\left\\langle{#1}\\middle|{#2}\\right\\rangle}$$\n",
    "\n",
    "$\\require{color}$\n",
    "# Linear Algebra Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda5252",
   "metadata": {},
   "source": [
    "### What is Linear Algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38365e",
   "metadata": {},
   "source": [
    "Linear algebra, simplistically, is the mathematics of linear equations. In particularly, linear algebra is concerned with vectors, objects belonging to vector spaces, and with operators that manipulate those vectors, which themselves are objects in a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f02f3",
   "metadata": {},
   "source": [
    "### Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b334a1e",
   "metadata": {},
   "source": [
    "A vector space is a set of elements, called vectors, that is closed under addition and scalar multiplication. \n",
    "\n",
    "An example vector in $\\mathbb{R}^3$ may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f61ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sp.Matrix([['x'],['y'],['z']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a77f04",
   "metadata": {},
   "source": [
    "To be \"closed\" under addition and scalar multiplication simply means that those actions performed on vectors within a vector space will yield vectors that remain in the same vector space. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,x,y,z,l = sp.symbols('a b c x y z lambda')\n",
    "\n",
    "A = sp.Matrix([[x],[y],[z]]) #change the x, y and z to numbers to play with different vectors\n",
    "B = sp.Matrix([[a],[b],[c]]) #change the a, b and c to numbers to play with different vectors\n",
    "\n",
    "#addition\n",
    "print('Addition of two vectors:')\n",
    "display(sp.Add(A,B))\n",
    "\n",
    "#scalar multiplication\n",
    "scalar = l # change this to a number to test different scalars\n",
    "print('Scalar multiplication of a vector:')\n",
    "display(A*scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c75884",
   "metadata": {},
   "source": [
    "For any real numbers plugged in above, the resulting vectors will remain in the vector space $\\mathbb{R}^3$, meaning this vector space is closed under addition and scalar multiplication.\n",
    "\n",
    "While the example above is specific for real numbers in three-dimensional space, vector spaces exist for vectors of dimension n and need not be constrained to the set of real numbers.\n",
    "\n",
    "An example vector of dimension n is:    $\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}$\n",
    "         \n",
    "Matrices can also be elements of a vector space.\n",
    "\n",
    "An important concept in linear algebra is the linear combination. For a set of vectors ${v_1},{v_2},...,{v_n}$, a linear combination of these vectors would be ${a_1}{v_1}+{a_2}{v_2}+...+{a_n}{v_n}$ where the set ${a_1},{a_2},...,{a_n}$ are coefficients. The resultant vector is in the same vector space as the original vectors, and is said to be a linear combination of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be76f8",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133085e",
   "metadata": {},
   "source": [
    "Matrices represent linear maps. A linear map is a transformation between different vector spaces that preserves closure under addition and scalar multiplication. \n",
    "\n",
    "Multiplying a vector by a matrix is a linear transformation by which the elements of the vector are mapped from one vector space to another through the operation of the matrix. Symbolically, for two vector spaces $\\mathbb{V}^m$ and $\\mathbb{V}^n$, an $m \\times n$ matrix $\\textbf{A}$ gives a linear transformation from $\\mathbb{V}^n$ to $\\mathbb{V}^m$ by mapping each vector $\\textbf{x}$ in $\\mathbb{V}^n$ to a vector $\\textbf{Ax}$ in $\\mathbb{V}^m$. Note that once again, the vector spaces are not necessarily real, or finite-dimensional.\n",
    "\n",
    "As a refresher, matrix multiplication with a vector (and other matrices) is performed by row-wise multiplication of the matrix with each column of the vector (or other matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c130b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,x,y = sp.symbols('a b c d e f g h x y')\n",
    "\n",
    "A = sp.Matrix([[a,b],[c,d]]) #Replace the letters with numbers to experiment with different matrices and vectors\n",
    "B = sp.Matrix([[e,f],[g,h]])\n",
    "\n",
    "V = sp.Matrix([[x],[y]])\n",
    "\n",
    "print('A:')\n",
    "display(A)\n",
    "\n",
    "print('B:')\n",
    "display(B)\n",
    "\n",
    "print('A multiplied with a vector V:')\n",
    "display(A@V)\n",
    "\n",
    "print('A multiplied with a matrix B:')\n",
    "display(A@B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d732f64",
   "metadata": {},
   "source": [
    "There are several important aspects of vectors and matrices:\n",
    "- Complex Conjugate\n",
    "- Transpose\n",
    "- Normality\n",
    "- Inner Products\n",
    "- Orthogonality\n",
    "- Outer Products\n",
    "- Matrix Commutation\n",
    "- Determinants\n",
    "- Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac2813",
   "metadata": {},
   "source": [
    "#### Complex Conjugate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745928e",
   "metadata": {},
   "source": [
    "For a vector, or matrix, composed of complex numbers, the complex conjugate replaces every instance of the imaginary number $i$ with $-i$ (Recall that $i$=$\\sqrt{-1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import I\n",
    "\n",
    "a,b,c,d,e,f,g,h,x,y = sp.symbols('a b c d e f g h x y',real=True)\n",
    "\n",
    "A = sp.Matrix([[a+I*b,c+I*d],[e+I*f,g+I*h]])\n",
    "print('Matrix A:')\n",
    "display(A)\n",
    "print('Complex conjugate matrix A:')\n",
    "display(sp.conjugate(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9676c4",
   "metadata": {},
   "source": [
    "#### Tranpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92925d9f",
   "metadata": {},
   "source": [
    "The transpose of a vector or matrix interchanges rows and columns.\n",
    "\n",
    "Notice that for a square matrix, the tranpose does not change the diagonal elements.\n",
    "\n",
    "A column vector will become a row vector, and an $m \\times n$ matrix will become a $n \\times m$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,j,k,x,y = sp.symbols('a b c d e f g h j k x y',real=True)\n",
    "\n",
    "A = sp.Matrix([[a+I*b,c+I*d],[e+I*f,g+I*h]])\n",
    "print('Matrix A:')\n",
    "display(A)\n",
    "print('Transpose matrix A:')\n",
    "display(A.T)\n",
    "\n",
    "B = sp.Matrix([[a+I*b],[c+I*d]])\n",
    "print('Vector B:')\n",
    "display(B)\n",
    "print('Transpose vector B:')\n",
    "display(B.T)\n",
    "\n",
    "C = sp.Matrix([[a+I*b,c+I*d],[e+I*f,g+I*h],[j+I*k,x+I*y]])\n",
    "print('3 x 2 matrix C:')\n",
    "display(C)\n",
    "print('2 x 3 transposed matrix C:')\n",
    "display(C.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf0b07",
   "metadata": {},
   "source": [
    "#### Dagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f275b",
   "metadata": {},
   "source": [
    "The dagger combines the above two actions, so complex conjugate tranpose is synonymous with dagger. Sometimes, you'll see just conjugate transpose, but it means the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289be458",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,j,k,x,y = sp.symbols('a b c d e f g h j k x y',real=True)\n",
    "\n",
    "def Dagger(v):\n",
    "    return sp.conjugate(v.T)\n",
    "\n",
    "A = sp.Matrix([[a+I*b,c+I*d],[e+I*f,g+I*h]])\n",
    "print('Matrix A:')\n",
    "display(A)\n",
    "print('Dagger matrix A:')\n",
    "display(Dagger(A))\n",
    "\n",
    "B = sp.Matrix([[a+I*b],[c+I*d]])\n",
    "print('Vector B:')\n",
    "display(B)\n",
    "print('Dagger vector B:')\n",
    "display(Dagger(B))\n",
    "\n",
    "C = sp.Matrix([[a+I*b,c+I*d],[e+I*f,g+I*h],[j+I*k,x+I*y]])\n",
    "print('3 x 2 matrix C:')\n",
    "display(C)\n",
    "print('2 x 3 transposed matrix C:')\n",
    "display(Dagger(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5479",
   "metadata": {},
   "source": [
    "#### Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc3021",
   "metadata": {},
   "source": [
    "Vectors of any dimension have an associated magnitude. In $\\mathbb{R}^3$, this is easily conceptualized as the length of the vector. The magnitude of a vector in higher dimensions is the generalization of the concept of length. The magnitude for a vector $v$=$\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}$ is given by the formula $\\lvert v \\rvert$=$\\sqrt{x_1^2+x_2^2+...+x_n^2}$.\n",
    "         \n",
    "A vector $v$ is considered to be normalized when its magnitude $\\lvert v \\rvert$ is equal to one.\n",
    "\n",
    "In order to normalize a vector, find its magnitude, and divide each element of the vector by that magnitude.\n",
    "\n",
    "Importantly, a normalized vector will point in the same direction as the same vector unnormalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = sp.Matrix([[2],[1],[3]]) # change these numbers to try different vectors\n",
    "\n",
    "print('Vector V:')\n",
    "display(V)\n",
    "\n",
    "print('Magnitude of vector V:')\n",
    "mag = sp.sqrt(V[0]**2+V[1]**2+V[2]**2)\n",
    "display(mag)\n",
    "\n",
    "Vnormed = V / mag\n",
    "print('Normalized vector V:')\n",
    "display(Vnormed)\n",
    "\n",
    "magnormed = sp.sqrt(Vnormed[0]**2+Vnormed[1]**2+Vnormed[2]**2)\n",
    "print('Magnitude of normalized vector V:')\n",
    "display(magnormed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01eeb0",
   "metadata": {},
   "source": [
    "#### Inner Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fe8e2",
   "metadata": {},
   "source": [
    "The inner product of two vectors of dimension n, real or complex, $v$ and $u$ is a scalar and is given by the formula $\\bra{u}\\ket{v}$=${u^\\dagger}v$=$\\Sigma_{i=1}^{n}u^\\dagger_iv_i$, where ${u^\\dagger}$ is the dagger of $u$.\n",
    "\n",
    "The inner product can be used to define the angle between two vectors and the magnitude of a vector, using the formulas $\\theta$=$arccos(\\dfrac{\\bra{u}\\ket{v}}{\\lvert v \\rvert \\lvert u \\rvert})$ and $\\lvert u \\rvert$=$\\sqrt{\\bra{u}\\ket{u}}$, respectively. Note that for the magnitude of a vector, the inner product is taken for the vector with its own dagger. This definition of the magnitude is equivalent to the one given above.\n",
    "\n",
    "To understand the inner product, consider the following example of two vectors in $\\mathbb{R}^2$ and the determination of the angle between them.\n",
    "\n",
    "While the example shown below is in two dimensions with real numbers, the definitions above extend to n dimensional vector spaces that can include complex numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17420949",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = sp.Matrix([[1/2],[sp.sqrt(3)/2]]) # Change these to experiment with different vectors.\n",
    "print('Vector U:')\n",
    "display(U)\n",
    "\n",
    "V = sp.Matrix([[1],[0]]) # Leave this one alone\n",
    "print('Vector V:')\n",
    "display(V)\n",
    "\n",
    "# Implementing the above equation for theta\n",
    "\n",
    "theta = float((sp.acos((U[0]*V[0]+U[1]*V[1])/(sp.sqrt(V[0]**2+V[1]**2)*sp.sqrt(U[0]**2+U[1]**2))))*57.2958)\n",
    "print(f'The angle between the two vectors: {theta:.2f}')\n",
    "\n",
    "# plotting stuff, ignore below this\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "plt.axis('off')\n",
    "plt.axhline(y = 0, color = 'black')\n",
    "plt.axvline(x = 0, color = 'black')\n",
    "\n",
    "origin = np.array([[0, 0],[0, 0]])\n",
    "W = np.array([[5*float(U[0]),5*float(U[1])], [5*float(V[0]),5*float(V[1])]])\n",
    "plt.quiver(*origin, W[:,0], W[:,1], color=['r','b','g'], scale=21)\n",
    "\n",
    "ax.annotate((f'Angle: {theta:.2f}'),xy=(0.5,0.475))\n",
    "\n",
    "plt.ylim(-4,4)\n",
    "plt.xlim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec413140",
   "metadata": {},
   "source": [
    "#### Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e8b82",
   "metadata": {},
   "source": [
    "Similar to how magnitude is a generalization of length from three dimensions to higher order dimensions, orthogonality extends the concept of perpendicularity in two dimensions to higher order dimensions.\n",
    "\n",
    "One way to think of the inner product is that it quantifies how much two vectors overlap. The inner product, therefore, provides a way to determine if two vectors are orthogonal. If the inner product of two vectors $u$ and $v$, $\\bra{u}\\ket{v}$=0, then the two vectors are considered orthogonal. \n",
    "\n",
    "To conceptualize this, play with the figure below by inputting several different vectors in $\\mathbb{R}^2$. Try to find two vectors that are orthogonal to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = sp.Matrix([[1/2],[1]]) # Change these to experiment with different vectors.\n",
    "print('Vector U:')\n",
    "display(U)\n",
    "\n",
    "V = sp.Matrix([[1],[0]]) # Change these to experiment with different vectors.\n",
    "print('Vector V:')\n",
    "display(V)\n",
    "\n",
    "# Implementing the above equation for theta\n",
    "\n",
    "theta = float((sp.acos((U[0]*V[0]+U[1]*V[1])/(sp.sqrt(V[0]**2+V[1]**2)*sp.sqrt(U[0]**2+U[1]**2))))*57.2958)\n",
    "print(f'The angle between the vectors: {theta:.2f}')\n",
    "\n",
    "# Calculating the overlap by taking the inner product of the two vectors\n",
    "\n",
    "inpdt = (U[0]*V[0]+U[1]*V[1])\n",
    "print(f'The overlap of the two vectors: {inpdt:.2f}')\n",
    "\n",
    "# plotting stuff, ignore below this\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "plt.axis('off')\n",
    "plt.axhline(y = 0, color = 'black')\n",
    "plt.axvline(x = 0, color = 'black')\n",
    "\n",
    "origin = np.array([[0, 0],[0, 0]])\n",
    "W = np.array([[5*float(U[0]),5*float(U[1])], [5*float(V[0]),5*float(V[1])]])\n",
    "plt.quiver(*origin, W[:,0], W[:,1], color=['r','b','g'], scale=21)\n",
    "\n",
    "ax.annotate((f'Angle: {theta:.2f}'),xy=(0.5,0.475))\n",
    "ax.annotate((f'Overlap: {inpdt:.2f}'),xy=(0.5,-0.475))\n",
    "\n",
    "plt.ylim(-4,4)\n",
    "plt.xlim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50071a41",
   "metadata": {},
   "source": [
    "If two vectors are orthogonal to one another, they can alse be said to be linearly independent of one another. A set of vectors is linearly independent if none of the vectors in the set can be written as a linear combination of other vectors within the set. Therefore, a good way to test for linear independence of vectors is to take the inner product of the vectors to test for orthogonality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342070c",
   "metadata": {},
   "source": [
    "#### Outer Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b85b4",
   "metadata": {},
   "source": [
    "Not sure if I'll include this. Might not be relevant to Chem 401."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d6c4e",
   "metadata": {},
   "source": [
    "#### Matrix Commutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0b1f2",
   "metadata": {},
   "source": [
    "Matrix multiplication is unique compared to more familiar methods of multiplication in that it is not necessarily commutative. We normally know that for two scalars $a$ and $b$, $ab$=$ba$. However, for matrices, this is not always true. \n",
    "\n",
    "The commutator of two matrices $A$ and $B$ is frequently represented by the following notation: $[A,B]$=$AB-BA$, where two matrices commute if $[A,B]$=0. Note that 0 in this case refers to a matrix with all elements equal to zero.\n",
    "\n",
    "The example below shows two matrices that commute, and two that do not commute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.Matrix([[0,1],[0,0]]) # Feel free to change the numbers. See if you can find other matrices that commute.\n",
    "print('Matrix A:')\n",
    "display(A)\n",
    "\n",
    "B = sp.Matrix([[1,0],[0,1]])\n",
    "print('Matrix B:')\n",
    "display(B)\n",
    "\n",
    "comm = A@B-B@A\n",
    "print('[A,B]:')\n",
    "display(comm)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "C = sp.Matrix([[0,1],[0,0]])\n",
    "print('Matrix C:')\n",
    "display(C)\n",
    "\n",
    "D = sp.Matrix([[1,0],[0,0]])\n",
    "print('Matrix D:')\n",
    "display(D)\n",
    "\n",
    "comm = C@D-D@C\n",
    "print('[C,D]:')\n",
    "display(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c3279",
   "metadata": {},
   "source": [
    "#### Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5af78f",
   "metadata": {},
   "source": [
    "The determinant of a square matrix is a function of the elements in the matrix, and it characterizes the matrix. Note that determinants do not exist for non-square matrices.\n",
    "\n",
    "For the determinant of a $2 \\times 2$ matrix, there exists a simple formula: if $A$=$\\begin{bmatrix}\n",
    "    a       & b \\\\\n",
    "    c       & d \\\\\n",
    "\\end{bmatrix}$, then $\\textbf{det}(A)$=$ad-bc$.\n",
    "\n",
    "For larger matrices, there exist different ways to find the determinant. The one that will be discussed here is a cofactor expansion.\n",
    "\n",
    "For simplicity, we'll consider a $3 \\times 3$ matrix, but the method works for larger matrices, too.\n",
    "\n",
    "The cofactor expansion works by picking a row or column, multiplying the numbers in the row or column by the determinants of the smaller $2 \\times 2$ matrices they cut out, and summing up the determinants. It's much clearer in symbols:\n",
    "\n",
    "To take the determinant of a matrix $A$=$\\begin{bmatrix}\n",
    "    a       & b & c\\\\\n",
    "    d       & e & f\\\\\n",
    "    g       & h & i\\\\\n",
    "\\end{bmatrix}$, first select a row. For this example, we'll select row 1. The cofactors in row 1 are $a$, $b$, and $c$. Next, assign them positive or negative signs according to the following formula: $C_{i,j}$=$(-1)^{i+j}$ where $i$ and $j$ are the matrix element indices. Visually, for a $3 \\times 3$ matrix, this is essentially telling you to assign a positive or negative sign based on the cofactors position in the matrix according to this image: $\\begin{bmatrix}\n",
    "    +       & - & +\\\\\n",
    "    -       & + & -\\\\\n",
    "    +       & - & +\\\\\n",
    "\\end{bmatrix}$. \n",
    "\n",
    "Next, take the determinants of the smaller $2 \\times 2$ matrices cut out by the cofactors. Visually, for cofactor $a$ in red, the $2 \\times 2$ determinant to take is shown in blue: $\\begin{bmatrix}\n",
    "    \\textcolor{red}{\\textbf{a}}       & b & c\\\\\n",
    "    d       & \\textcolor{darkblue}{\\textbf{e}} & \\textcolor{darkblue}{\\textbf{f}}\\\\\n",
    "    g       & \\textcolor{darkblue}{\\textbf{h}} & \\textcolor{darkblue}{\\textbf{i}}\\\\\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Similarly, for the next cofactor in row 1, $b$, the correct $2 \\times 2$ determinant is shown: $\\begin{bmatrix}\n",
    "    a       & \\textcolor{red}{\\textbf{b}} & c\\\\\n",
    "    \\textcolor{darkblue}{\\textbf{d}}       & e & \\textcolor{darkblue}{\\textbf{f}}\\\\\n",
    "    \\textcolor{darkblue}{\\textbf{g}}       & h & \\textcolor{darkblue}{\\textbf{i}}\\\\\n",
    "\\end{bmatrix}$. Finally, for the third cofactor in row 1, $c$, the correct $2 \\times 2$ determinant is shown: $\\begin{bmatrix}\n",
    "    a       & b & \\textcolor{red}{\\textbf{c}}\\\\\n",
    "    \\textcolor{darkblue}{\\textbf{d}}       & \\textcolor{darkblue}{\\textbf{e}} & f\\\\\n",
    "    \\textcolor{darkblue}{\\textbf{g}}       & \\textcolor{darkblue}{\\textbf{h}} & i\\\\\n",
    "\\end{bmatrix}$. Recall that each cofactor is multiplied by the correct sign according to the matrix above with $+$ and $-$ elements.\n",
    "\n",
    "Combining all of the above, the formula we arrive at for the determinant of the $3 \\times 3$ matrix using a cofactor expansion along row 1 is $+a(ei-fh)-b(di-fg)+c(dh-eg)$. Recall that this equation for the determinant of the matrix is not unique; a cofactor expansion will work along any row or any column of the matrix.\n",
    "\n",
    "An example calculation of the determinant of a $3 \\times 3$ matrix is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd07e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.Matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print('Matrix A:')\n",
    "display(A)\n",
    "\n",
    "# Try selecting a different row or column and computing the determinant\n",
    "# The determinant should always be equal to 0 for this particular matrix\n",
    "\n",
    "det = +1*(5*9-6*8)-2*(4*9-6*7)+3*(4*8-5*7) # modify these numbers to select a different row or column\n",
    "print(f'The determinant of Matrix A is: {det}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc72b5",
   "metadata": {},
   "source": [
    "The above method of cofactor expansion can be used for higher dimension matrices. Each smaller submatrix would require a cofactor expansion, making the calculation more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229b513",
   "metadata": {},
   "source": [
    "#### Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ca61a",
   "metadata": {},
   "source": [
    "Recall that a matrix is defined above as a linear map. That is, it exacts some transformation on a vector to turn it into another vector. \n",
    "\n",
    "Consider a matrix $A$=$\\begin{bmatrix}\n",
    "    0       & -2 \\\\\n",
    "    -4       & 2 \\\\\n",
    "\\end{bmatrix}$ and its action on the vectors $u$=$\\begin{bmatrix}\n",
    "    1       \\\\\n",
    "    1       \\\\\n",
    "\\end{bmatrix}$ and $v$=$\\begin{bmatrix}\n",
    "    -1       \\\\\n",
    "    1       \\\\\n",
    "\\end{bmatrix}$: \n",
    "\n",
    "$Au$=$\\begin{bmatrix}\n",
    "    0       & -2 \\\\\n",
    "    -4       & 2 \\\\\n",
    "\\end{bmatrix}$$\\begin{bmatrix}\n",
    "    1       \\\\\n",
    "    1       \\\\\n",
    "\\end{bmatrix}$=$\\begin{bmatrix}\n",
    "    -2       \\\\\n",
    "    -2       \\\\\n",
    "\\end{bmatrix}$=$-2$$\\begin{bmatrix}\n",
    "    1       \\\\\n",
    "    1       \\\\\n",
    "\\end{bmatrix}$ and $Av$=$\\begin{bmatrix}\n",
    "    0       & -2 \\\\\n",
    "    -4       & 2 \\\\\n",
    "\\end{bmatrix}$$\\begin{bmatrix}\n",
    "    -1       \\\\\n",
    "    1       \\\\\n",
    "\\end{bmatrix}$=$\\begin{bmatrix}\n",
    "    -2       \\\\\n",
    "    6       \\\\\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Notice that $Au$=$-2u$, while $Av$ does not equal a scalar times $v$.\n",
    "\n",
    "Since the action of $A$ on $u$ yields the vector $u$ multiplied by a scalar, $u$ is called an eigenvector of $A$, and the scalar it is multiplied by, $\\lambda$, is called its eigenvalue. In the example above, $\\lambda$=$-2$ for $u$.\n",
    "\n",
    "$v$ is not an eigenvector of $A$ since action of $A$ on $v$ does not yield $\\lambda v$, and therefore there are no eigenvalues.\n",
    "\n",
    "How do we find the eigenvalues of a matrix if we don't know the eigenvectors? \n",
    "\n",
    "If we consider a matrix $W$ and one of its eigenvectors, $x$, with the associated eigenvalue, $\\lambda$, the equation $Wx$=$\\lambda x$ is true. We can rearrange it as $(\\lambda I-W)x$=$0$, where $I$ is the identity matrix and $x$ is a nonzero vector. From this, it follows that $\\textbf{det}(\\lambda I-W)$=$0$. By solving this equation for $\\lambda$, one can find the eigenvalues of the matrix $W$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
